{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e10377-321e-4c42-b2a8-20df8b5927a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f323cc-31d0-41ab-aef2-8848a10f7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Team 5 Analytics Challenge Group Assignment: Data Wrangling \n",
    "\n",
    "Authors: Andrew Bakker, Hutson Collins, Durrelle Maynard, Dario Santiago Lopez, and Chris Sawyer \n",
    "Collaborators: ChatGPT (OpenAI), which assisted in iterative troubleshooting, design \n",
    "decisions, and refining data-cleaning logic.\n",
    "\n",
    "Beginning of Assignment: \n",
    "The first thing we want to do is import the libraries necessary for the given task \n",
    "In this case, we import pandas since we will be analyzing and creating dataframes, and any additional libraries \n",
    "that immediately comes to mind \n",
    "\"\"\"\n",
    "# --- Step 1: Import libraries ---\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a1523-996b-4800-9f1a-5a8ab066fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Since this is a team assignment, we will be implementing a way for every teammate to access the CSV files without \n",
    "directly asking for the file path where the CSV files are located. Instead, if they are already in the same directory, \n",
    "they can just start running the code \n",
    "\"\"\"\n",
    "\n",
    "# --- Step 2: Open/Analyze the CSV files & Create DF's ---\n",
    "# Get the directory where the current notebook is located\n",
    "notebook_dir = Path().resolve()\n",
    "\n",
    "# Point to CSV in the same directory\n",
    "ar_csv = notebook_dir / \"application-records.csv\" # \"ar\" is application-records.csv abbreviated, so we don't have to type out a long variable \n",
    "pr_csv = notebook_dir / \"personal-records.csv\"    # \"pr\" is personal-records.csv abbreviated for the same reason above \n",
    "\n",
    "# Create the DataFrame  \n",
    "ar_df = pd.read_csv(ar_csv)\n",
    "pr_df = pd.read_csv(pr_csv)\n",
    "\n",
    "# Take a quick peek of the sizes/shapes \n",
    "print(\"Shape Preview\")\n",
    "print(\"----------------------\")\n",
    "print(\"application-records.csv size: \", ar_df.shape)\n",
    "print(\"personal-records.csv size: \", pr_df.shape)\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "# Now take a look at the actual dataframes (do them in two different cell blocks)\n",
    "print(\"Now Previewing DataFrames\")\n",
    "print(\"application-records.csv\")\n",
    "ar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240efbc7-10f5-4e09-b982-fc08371e8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at the other DataFrame\n",
    "print(\"personal-records.csv\")\n",
    "pr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471136b4-fe3f-4a9e-a43d-30c83349280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if they have any columns in common \n",
    "print(\"Column Names\")\n",
    "print(\"application-records.csv columns:\")\n",
    "print(list(ar_df.columns))\n",
    "print(\"\\npersonal-records.csv columns:\")\n",
    "print(list(pr_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad05d4a-29a0-441b-b041-d3052e8196d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "So, the only columns the two datasets share is record_id. However, personal-records.csv has significantly more rows than \n",
    "application-records.csv. So, my (Dario's) thought process is to see if they have any duplicates in either of the datasets \n",
    "since I would consider cleaning pr_df (since it has 6 rows) and then consider merging them into one dataset. \n",
    "\"\"\" \n",
    "# Check for duplicates in both 'record_id' columns in each of the datasets \n",
    "print(\"AR duplicate IDs:\", ar_df['record_id'].duplicated().sum())\n",
    "print(\"PR duplicate IDs:\", pr_df['record_id'].duplicated().sum())\n",
    "\n",
    "# Also check for shared, extra, and total length of the column \n",
    "ar_ids = set(ar_df['record_id'])\n",
    "pr_ids = set(pr_df['record_id'])\n",
    "\n",
    "print(\"\\nAR IDs:\", len(ar_ids))\n",
    "print(\"PR IDs:\", len(pr_ids))\n",
    "print(\"Shared IDs:\", len(ar_ids & pr_ids))\n",
    "print(\"Extra in PR:\", len(pr_ids - ar_ids))\n",
    "print(\"Extra in AR:\", len(ar_ids - pr_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf88fa-7a9e-4641-b492-846b82a7355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze the duplicates to help with how we will move forward. Specifically, look at the values in the duplicates\"\"\" \n",
    "# Look at a few duplicate examples in AR\n",
    "ar_dupes = ar_df[ar_df['record_id'].duplicated(keep=False)]\n",
    "print(\"AR duplicate sample:\")\n",
    "display(ar_dupes.sort_values('record_id').head(10))\n",
    "\n",
    "# Look at a few duplicate examples in PR\n",
    "pr_dupes = pr_df[pr_df['record_id'].duplicated(keep=False)]\n",
    "print(\"PR duplicate sample:\")\n",
    "display(pr_dupes.sort_values('record_id').head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4da9ed-d98d-4a38-a4aa-23c79d0cb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "So, based on the results above we can see that the 'duplicates' are not actually duplicates, but rather just NaNs. \n",
    "So, I (Dario) think it would be best to go on and drop them since they are missing their unique identifier. \n",
    "From there, down the road, the additional 'record_id' values that aren't found in application-records.csv \n",
    "will be dropped during an inner join since they are unmatched. I think this would be the best first step in \n",
    "the data wrangling/cleaning process since we would just be removing a bunch of noisy rows. \n",
    "\"\"\"\n",
    "# --- Step 3: Remove rows with missing record_id ---\n",
    "\n",
    "# Create variables to help show the shape of both DFs (before dropping duplicates) \n",
    "before_ar = ar_df.shape[0]\n",
    "before_pr = pr_df.shape[0]\n",
    "\n",
    "# Drop duplicates\n",
    "ar_df = ar_df.dropna(subset=['record_id']).copy()\n",
    "pr_df = pr_df.dropna(subset=['record_id']).copy()\n",
    "\n",
    "print(f\"AR rows removed: {before_ar - ar_df.shape[0]}\")\n",
    "print(f\"PR rows removed: {before_pr - pr_df.shape[0]}\")\n",
    "\n",
    "# Confirm that there are no more duplicates \n",
    "print(\"AR duplicate IDs after drop:\", ar_df['record_id'].duplicated().sum())\n",
    "print(\"PR duplicate IDs after drop:\", pr_df['record_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a52dbe-569f-4df7-b149-ecd945077f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick peek at both df shapes again and ensure we actually removed the duplicates\n",
    "print(\"AR Shape: \", ar_df.shape)\n",
    "print(\"PR Shape: \", pr_df.shape)\n",
    "print(\"\\nUnique AR ID's:\", ar_df['record_id'].nunique())\n",
    "print(\"Unique PR ID's:\", pr_df['record_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c294308-8614-4aec-96e6-1628b47f2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner join of both dataframes\n",
    "df_records = pd.merge(ar_df, pr_df, on='record_id',how = 'inner')\n",
    "df_records.to_csv(\"df_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaeb2dc-4cdb-4431-8bb7-6a75dee77690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Ebay csv file\n",
    "df_records = pd.read_csv(\"df_records.csv\")\n",
    "#standardize column names\n",
    "df_records.columns = df_records.columns.str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248970b-c055-4302-a8ee-2d7c2a24ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records.head(20)\n",
    "df_records.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166165f-8968-42f6-8831-95060ea79d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view all initial columns\n",
    "for col in df_records.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5801e2b-f72e-4b6a-bdf5-d0a7974aefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missingness in data (as percentages)\n",
    "missing_percentages = (df_records.isnull().sum() / len(df_records)) * 100\n",
    "\n",
    "# filter only columns with more than 1% (adjust as needed)\n",
    "missing_percentages = missing_percentages[missing_percentages > 1]\n",
    "\n",
    "# sort in descending order\n",
    "missing_percentages = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec0fc8-8fbd-4639-b2ec-09c6116e0d66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean Income Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b2e0f-b44a-4411-9863-af1ae26fe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Income Tab\n",
    "income = df_records['income'].tolist()\n",
    "income\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506354a-3ae3-4a0f-ae70-4688f70c835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For standardization of the income column, we are limitiing all values to 2 decimal points. \n",
    "#Apply change to df_records\n",
    "df_records['income'] = df_records['income'].round(2)\n",
    "income = df_records['income'].tolist()\n",
    "income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e727d-1d8c-476b-ab6a-1131a8cd706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many instances of each unique value\n",
    "income_counts = df_records['income'].value_counts()\n",
    "\n",
    "print(income_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012190e-0b6a-48fc-8b9c-de0004531029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are checking empty values inn our income column\n",
    "income_nan_count = df_records['income'].isnull().sum()\n",
    "print(income_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e00fc5-2a3c-400e-9658-9d8341b79e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Distribution of income (quantile variable) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "df_records['income'].hist(bins=50, edgecolor='black')\n",
    "plt.title(\"Distribution of income (quantiles)\")\n",
    "plt.xlabel(\"income (0–1 scale)\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()\n",
    "\n",
    "# --- Distribution of salary (raw dollars) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "df_records['salary'].hist(bins=50, edgecolor='black')\n",
    "plt.title(\"Distribution of salary (USD)\")\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()\n",
    "\n",
    "# --- Relationship between income (quantile) and salary ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(df_records['income'], df_records['salary'], alpha=0.2, s=10)\n",
    "plt.title(\"income (quantile) vs salary (USD)\")\n",
    "plt.xlabel(\"income (0–1)\")\n",
    "plt.ylabel(\"salary (USD)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e45e6-677f-409f-98d9-04b28e1b8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_nan = df_records[df_records['income'].isnull()]\n",
    "income_nan.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f817b-7c58-4c75-9a21-b3ec962f8589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean name_email_similarity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8740c3f-d9c5-4093-9fcf-9c0e51035ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "name_email_similarity = df_records['name_email_similarity'].tolist()\n",
    "name_email_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25cdd8-151d-4db5-b856-96f207be5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For standardization of the name_email_similarity column, we are limitiing all values to 2 decimal points. \n",
    "#Apply change to df_records\n",
    "df_records['name_email_similarity'] = df_records['name_email_similarity'].round(2)\n",
    "name_email_similarity = df_records['name_email_similarity'].unique()\n",
    "name_email_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f63258f-f5a8-4d96-8cc6-8a4857ca234f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean Salary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d3b12-9380-4912-be37-19b22018864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "salary = df_records['salary'].tolist()\n",
    "salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647450b4-99a1-49bb-9b0d-6a65ea794b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intial view of the salary column does not show any discrepancies\n",
    "# For standardization of the salary column, we are ensuring there are no values that have cents included in the submission\n",
    "#Apply change to df_records\n",
    "df_records['salary'] = df_records['salary'].round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235b05b-3c8a-4c07-b2f5-714246a352df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean prev_address_months_count Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47164bba-161a-43df-929e-3446ec174f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "prev_address_months_count = df_records['prev_address_months_count'].tolist()\n",
    "prev_address_months_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be0e00-134d-4fb4-adeb-73d851e0823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Missingness in the table\n",
    "count_neg_one_missing = (df_records['prev_address_months_count'] == -1).sum()\n",
    "missing_percentages = (count_neg_one_missing / len(df_records)) * 100\n",
    "print(f\"Number of -1 values:\", count_neg_one_missing)\n",
    "print(f\"Percentage of -1 values: {missing_percentages:.2f}%\")\n",
    "# check for missingness in data (as percentages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e9b11-7d3a-4475-ab8b-49c194b84891",
   "metadata": {},
   "source": [
    "The column has about 71% of values missing; however, this is an optional tab, filled out by the applicants, as indicated by Martin. We will keep this column in. It probably is not a strong indicator on its own, but it may point to other red flags in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac7c2f-2579-40b6-8290-6ac06074b91b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean current_address_months_count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c076a-751f-4b74-ad49-af267685f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "current_address_months_count = df_records['current_address_months_count'].unique()\n",
    "current_address_months_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cc03e-891d-4a49-95f7-0a1c00332f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Missingness in the table\n",
    "count_neg_one_missing = (df_records['current_address_months_count'] == -1).sum()\n",
    "missing_percentages = (count_neg_one_missing / len(df_records)) * 100\n",
    "print(f\"Number of -1 values:\", count_neg_one_missing)\n",
    "print(f\"Percentage of -1 values: {missing_percentages:.2f}%\")\n",
    "# check for missingness in data (as percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9181347-fa2e-4b06-a315-741a995d7b23",
   "metadata": {},
   "source": [
    "There are not many missing values in this values. We can possibly impute these and create flag variables for those. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af1d1b-b8d5-4d85-8fde-ff80b3acec59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean customer_age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185204b7-b746-4bcc-b322-d726da51624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "customer_age = df_records['customer_age'].unique()\n",
    "customer_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba0a94-00ea-490c-89e9-b3aea784bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Missingness in the table\n",
    "count_missing = (df_records['customer_age'].isnull()).sum()\n",
    "missing_percentages = (count_neg_one_missing / len(df_records)) * 100\n",
    "print(f\"Number of missing values:\", count_neg_one_missing)\n",
    "print(f\"Percentage of missing values: {missing_percentages:.2f}%\")\n",
    "# check for missingness in data (as percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6057a-9dfd-43dc-8cfd-2d7741d4e17c",
   "metadata": {},
   "source": [
    "There is nothing missing in this column. There are also no values that do not represent the age bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceacafb-4959-4de8-a4c2-3de65000d6d3",
   "metadata": {},
   "source": [
    "# Clean date_of_birth column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c5cae952-4663-4eaa-a64e-ba64fb86d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_values = df_records[['date_of_birth']]\n",
    "\n",
    "# Save to CSV\n",
    "birth_values.to_csv(\"birth_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "26053bd1-7dc5-4b21-b103-5b3db35731a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 823551/823551 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def standardize_date_column(df, col, output_fmt=\"%d-%m-%Y\", dayfirst_preferred=True,\n",
    "                            two_digit_year_pivot=2025, keep_raw=True, report=True):\n",
    "    \"\"\"\n",
    "    Clean & standardize a messy date column into a single string format (default dd-mm-yyyy).\n",
    "\n",
    "    Handles:\n",
    "      - Mixed delimiters: 18-12-2003, 18/09/94\n",
    "      - Month names: 21-Jun-76, 5 Dec 72\n",
    "      - U.S. style leftovers: 1/26/87\n",
    "      - Excel serials: 45123\n",
    "      - Compact 8-digit numbers: 20240531 or 31052024\n",
    "      - Two-digit years with pivot correction (e.g., 87 -> 1987 if parsed as 2087)\n",
    "\n",
    "    Args:\n",
    "      df: DataFrame\n",
    "      col: column name containing messy dates (string-like)\n",
    "      output_fmt: final strftime format\n",
    "      dayfirst_preferred: first try day-first parsing (common outside U.S.)\n",
    "      two_digit_year_pivot: years > pivot after parsing (from 2-digit inputs) will be shifted -100 years\n",
    "      keep_raw: if True, adds `{col}_raw` with original text\n",
    "      report: if True, prints a quick parsing summary\n",
    "\n",
    "    Returns:\n",
    "      Series of strings in output_fmt with NaN for unparsed.\n",
    "    \"\"\"\n",
    "    s_raw = df[col].astype(str).str.strip()\n",
    "    s_raw = s_raw.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "    if keep_raw:\n",
    "        raw_col = f\"{col}_raw\"\n",
    "        if raw_col not in df.columns:\n",
    "            df[raw_col] = s_raw\n",
    "\n",
    "    # Working datetime series\n",
    "    out = pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    def try_parse(mask, parser, **kwargs):\n",
    "        nonlocal out\n",
    "        idx = out[mask].index\n",
    "        if len(idx) == 0:\n",
    "            return\n",
    "        parsed = parser(s_raw.loc[idx], errors=\"coerce\", **kwargs)\n",
    "        out.loc[idx] = out.loc[idx].fillna(parsed)\n",
    "\n",
    "    # PASS 1: broad parse with preferred day-first\n",
    "    try_parse(out.isna(), pd.to_datetime, dayfirst=dayfirst_preferred)\n",
    "\n",
    "    # PASS 2: named-month formats (explicit)\n",
    "    named_formats = [\"%d-%b-%y\", \"%d-%b-%Y\", \"%d %b %y\", \"%d %b %Y\",\n",
    "                     \"%d-%B-%y\", \"%d-%B-%Y\", \"%d %B %y\", \"%d %B %Y\"]\n",
    "    for fmt in named_formats:\n",
    "        mask = out.isna()\n",
    "        try_parse(mask, lambda x, **k: pd.to_datetime(x, format=fmt, **k))\n",
    "\n",
    "    # PASS 3: explicit D/M/Y (two- and four-digit years)\n",
    "    dmy_formats = [\"%d/%m/%y\", \"%d/%m/%Y\", \"%d-%m-%y\", \"%d-%m-%Y\"]\n",
    "    for fmt in dmy_formats:\n",
    "        mask = out.isna()\n",
    "        try_parse(mask, lambda x, **k: pd.to_datetime(x, format=fmt, **k))\n",
    "\n",
    "    # PASS 4: explicit M/D/Y for leftovers (U.S.-style)\n",
    "    mdy_formats = [\"%m/%d/%y\", \"%m/%d/%Y\", \"%m-%d-%y\", \"%m-%d-%Y\"]\n",
    "    for fmt in mdy_formats:\n",
    "        mask = out.isna()\n",
    "        try_parse(mask, lambda x, **k: pd.to_datetime(x, format=fmt, **k))\n",
    "\n",
    "    # PASS 5: compact 8-digit numbers (YYYYMMDD then DDMMYYYY)\n",
    "    mask = out.isna()\n",
    "    compact = s_raw.str.fullmatch(r\"\\d{8}\", na=False) & mask\n",
    "    if compact.any():\n",
    "        # Try YYYYMMDD\n",
    "        idx = compact[compact].index\n",
    "        parsed = pd.to_datetime(s_raw.loc[idx], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        # Fill successes\n",
    "        out.loc[idx] = out.loc[idx].fillna(parsed)\n",
    "        # For any still NaT, try DDMMYYYY\n",
    "        still = out.loc[idx].isna()\n",
    "        if still.any():\n",
    "            idx2 = out.loc[idx][still].index\n",
    "            parsed2 = pd.to_datetime(s_raw.loc[idx2], format=\"%d%m%Y\", errors=\"coerce\")\n",
    "            out.loc[idx2] = out.loc[idx2].fillna(parsed2)\n",
    "\n",
    "    # PASS 6: Excel serials (simple guard: 4–6 digits; adjust if needed)\n",
    "    mask = out.isna()\n",
    "    serial_like = s_raw.str.fullmatch(r\"\\d{4,6}\", na=False) & mask\n",
    "    if serial_like.any():\n",
    "        idx = serial_like[serial_like].index\n",
    "        serial_vals = pd.to_numeric(s_raw.loc[idx], errors=\"coerce\")\n",
    "        parsed = pd.to_datetime(serial_vals, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "        out.loc[idx] = out.loc[idx].fillna(parsed)\n",
    "\n",
    "    # --- Two-digit year pivot correction ---\n",
    "    # Detect rows whose raw value had a 2-digit year\n",
    "    two_digit_pattern = re.compile(r\"(^|[^0-9])\\d{1,2}[-/\\s](?:[A-Za-z]{3,}|[0-9]{1,2})[-/\\s]\\d{2}($|[^0-9])\")\n",
    "    has_two_digit_year = s_raw.fillna(\"\").apply(lambda x: bool(two_digit_pattern.search(x)))\n",
    "    # If parsed and year > pivot, subtract 100 years (only for two-digit-year inputs)\n",
    "    mask = out.notna() & has_two_digit_year & (out.dt.year > two_digit_year_pivot)\n",
    "    if mask.any():\n",
    "        out.loc[mask] = out.loc[mask] - pd.offsets.DateOffset(years=100)\n",
    "\n",
    "    # Final string formatting\n",
    "    result = out.dt.strftime(output_fmt)\n",
    "\n",
    "    # Optional report\n",
    "    if report:\n",
    "        total = len(df)\n",
    "        parsed = out.notna().sum()\n",
    "        print(f\"Parsed: {parsed}/{total} ({parsed/total*100:.2f}%)\")\n",
    "        if parsed < total:\n",
    "            unparsed = df.loc[out.isna(), col].head(10).tolist()\n",
    "            print(\"Sample unparsed values (first 10):\", unparsed)\n",
    "\n",
    "    return result\n",
    "\n",
    "# ----------------------\n",
    "# Usage\n",
    "# ----------------------\n",
    "df_records['date_of_birth'] = standardize_date_column(df_records, 'date_of_birth', output_fmt='%d-%m-%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "47a2196e-013a-40af-a368-3cece0a8930f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18-12-2003',\n",
       " '28-05-1993',\n",
       " '21-06-1976',\n",
       " '18-09-1994',\n",
       " '05-12-1981',\n",
       " '05-02-1982',\n",
       " '12-08-1988',\n",
       " '21-09-1995',\n",
       " '05-12-1972',\n",
       " '15-08-1984',\n",
       " '15-03-2004',\n",
       " '26-01-1987',\n",
       " '24-06-1993',\n",
       " '30-04-1982',\n",
       " '18-03-1997',\n",
       " '27-11-1990',\n",
       " '24-09-2002',\n",
       " '16-11-2002',\n",
       " '05-06-2000',\n",
       " '15-12-1990',\n",
       " '04-05-1968',\n",
       " '13-05-1984',\n",
       " '30-04-1986',\n",
       " '25-12-1977',\n",
       " '21-08-1981',\n",
       " '24-01-1996',\n",
       " '12-02-1979',\n",
       " '24-04-1987',\n",
       " '17-09-1978',\n",
       " '10-02-1982',\n",
       " '09-06-1980',\n",
       " '13-01-2001',\n",
       " '09-02-2000',\n",
       " '27-07-1998',\n",
       " '17-02-2000',\n",
       " '03-09-1988',\n",
       " '01-02-1964',\n",
       " '26-07-1975',\n",
       " '27-07-1975',\n",
       " '24-10-2001',\n",
       " '05-11-1995',\n",
       " '02-01-1970',\n",
       " '21-01-2003',\n",
       " '30-05-1996',\n",
       " '22-08-1978',\n",
       " '14-12-1991',\n",
       " '20-03-1987',\n",
       " '09-04-2001',\n",
       " '10-12-2003',\n",
       " '05-05-1988',\n",
       " '27-04-2001',\n",
       " '21-04-1969',\n",
       " '23-04-1982',\n",
       " '28-06-1990',\n",
       " '09-02-1994',\n",
       " '16-10-1970',\n",
       " '01-10-1996',\n",
       " '07-01-1979',\n",
       " '04-04-1995',\n",
       " '30-08-1999',\n",
       " '26-09-1993',\n",
       " '07-06-2003',\n",
       " '02-06-1999',\n",
       " '10-01-2003',\n",
       " '08-02-1972',\n",
       " '10-12-1993',\n",
       " '04-05-1983',\n",
       " '29-06-1990',\n",
       " '06-08-1985',\n",
       " '19-03-1995',\n",
       " '08-05-2001',\n",
       " '27-01-1974',\n",
       " '17-02-1984',\n",
       " '17-04-1927',\n",
       " '08-01-1997',\n",
       " '01-02-1999',\n",
       " '12-07-1983',\n",
       " '08-05-1990',\n",
       " '10-09-1996',\n",
       " '15-05-2001',\n",
       " '27-05-1958',\n",
       " '04-04-1994',\n",
       " '06-12-1971',\n",
       " '05-10-1992',\n",
       " '07-02-1999',\n",
       " '15-07-1975',\n",
       " '21-01-1975',\n",
       " '10-01-1983',\n",
       " '09-03-2005',\n",
       " '24-03-1993',\n",
       " '24-09-1977',\n",
       " '06-01-1999',\n",
       " '17-03-1985',\n",
       " '26-10-1999',\n",
       " '01-02-2002',\n",
       " '29-04-1993',\n",
       " '03-12-2002',\n",
       " '10-07-2003',\n",
       " '26-08-1991',\n",
       " '20-05-1977',\n",
       " '18-01-1939',\n",
       " '05-01-1988',\n",
       " '28-08-1983',\n",
       " '27-04-1987',\n",
       " '24-03-1994',\n",
       " '24-02-1960',\n",
       " '20-10-1981',\n",
       " '04-08-2001',\n",
       " '07-02-1986',\n",
       " '08-12-1979',\n",
       " '12-03-1978',\n",
       " '10-04-1999',\n",
       " '22-08-1988',\n",
       " '20-12-1976',\n",
       " '10-08-1983',\n",
       " '08-12-1982',\n",
       " '04-07-1961',\n",
       " '03-02-1998',\n",
       " '29-04-1994',\n",
       " '08-09-1995',\n",
       " '26-12-1997',\n",
       " '06-07-2002',\n",
       " '26-03-2004',\n",
       " '05-06-1982',\n",
       " '01-01-1987',\n",
       " '08-06-1970',\n",
       " '23-04-1984',\n",
       " '31-08-1974',\n",
       " '11-11-1973',\n",
       " '05-08-1993',\n",
       " '22-12-1975',\n",
       " '03-04-1980',\n",
       " '09-01-1978',\n",
       " '18-01-1946',\n",
       " '10-09-1995',\n",
       " '05-07-2012',\n",
       " '31-05-1981',\n",
       " '13-12-2000',\n",
       " '22-07-1967',\n",
       " '06-12-1966',\n",
       " '19-04-1990',\n",
       " '20-08-1979',\n",
       " '26-02-1970',\n",
       " '15-03-1989',\n",
       " '25-06-1994',\n",
       " '22-05-1997',\n",
       " '11-02-1977',\n",
       " '30-08-1993',\n",
       " '13-02-1976',\n",
       " '14-08-2003',\n",
       " '17-08-1981',\n",
       " '04-08-1971',\n",
       " '18-05-1989',\n",
       " '14-12-1974',\n",
       " '15-10-1987',\n",
       " '03-10-1962',\n",
       " '15-04-1998',\n",
       " '07-11-1982',\n",
       " '08-12-1969',\n",
       " '01-02-1978',\n",
       " '18-03-1995',\n",
       " '19-05-1979',\n",
       " '12-01-2002',\n",
       " '21-09-1977',\n",
       " '02-06-1985',\n",
       " '20-07-1979',\n",
       " '28-07-1993',\n",
       " '08-01-2003',\n",
       " '29-07-1979',\n",
       " '23-12-1990',\n",
       " '16-08-1993',\n",
       " '02-07-1980',\n",
       " '04-06-1988',\n",
       " '19-01-1972',\n",
       " '16-09-1972',\n",
       " '31-08-1986',\n",
       " '23-10-1990',\n",
       " '19-06-2003',\n",
       " '20-09-1975',\n",
       " '03-09-1995',\n",
       " '26-06-1983',\n",
       " '20-08-2004',\n",
       " '11-10-1961',\n",
       " '29-03-1959',\n",
       " '23-09-2003',\n",
       " '21-09-1982',\n",
       " '08-08-2000',\n",
       " '07-02-1992',\n",
       " '23-11-1979',\n",
       " '24-08-1997',\n",
       " '11-09-1998',\n",
       " '25-12-1979',\n",
       " '10-09-2001',\n",
       " '18-03-1962',\n",
       " '23-06-1996',\n",
       " '13-05-2002',\n",
       " '03-06-1995',\n",
       " '10-09-1986',\n",
       " '08-02-1997',\n",
       " '26-11-1967',\n",
       " '18-01-1990',\n",
       " '04-06-1976',\n",
       " '15-01-2002',\n",
       " '20-08-1983',\n",
       " '03-07-1973',\n",
       " '04-08-2000',\n",
       " '13-03-1975',\n",
       " '05-08-1995',\n",
       " '15-12-1978',\n",
       " '15-05-1977',\n",
       " '08-01-1982',\n",
       " '18-06-2002',\n",
       " '12-01-1987',\n",
       " '30-01-2002',\n",
       " '13-04-1975',\n",
       " '08-06-1979',\n",
       " '24-12-1984',\n",
       " '27-06-1984',\n",
       " '28-01-1984',\n",
       " '05-03-1995',\n",
       " '04-02-2001',\n",
       " '17-12-1992',\n",
       " '05-11-1984',\n",
       " '26-01-2003',\n",
       " '04-07-1989',\n",
       " '14-05-1969',\n",
       " '01-09-1970',\n",
       " '30-08-1968',\n",
       " '22-03-2000',\n",
       " '22-03-1979',\n",
       " '31-05-1981',\n",
       " '28-05-2001',\n",
       " '31-05-1985',\n",
       " '02-06-1984',\n",
       " '28-07-1976',\n",
       " '07-03-1957',\n",
       " '02-01-2013',\n",
       " '21-03-1996',\n",
       " '04-12-1983',\n",
       " '05-07-1982',\n",
       " '09-06-1988',\n",
       " '02-09-1964',\n",
       " '01-03-1985',\n",
       " '25-05-1987',\n",
       " '04-05-1977',\n",
       " '11-10-1998',\n",
       " '12-08-1999',\n",
       " '21-02-1988',\n",
       " '06-12-1982',\n",
       " '03-07-1982',\n",
       " '28-04-1982',\n",
       " '05-01-1999',\n",
       " '03-06-1993',\n",
       " '01-08-2000',\n",
       " '01-11-1992',\n",
       " '23-01-1989',\n",
       " '04-08-2010',\n",
       " '13-08-2004',\n",
       " '20-06-1989',\n",
       " '23-05-1968',\n",
       " '24-08-1984',\n",
       " '17-01-1976',\n",
       " '20-11-2001',\n",
       " '08-01-1996',\n",
       " '30-10-1967',\n",
       " '28-06-1987',\n",
       " '26-08-1977',\n",
       " '06-09-1974',\n",
       " '02-07-1992',\n",
       " '29-07-1971',\n",
       " '22-11-1986',\n",
       " '30-03-2005',\n",
       " '17-01-1996',\n",
       " '27-04-2000',\n",
       " '30-08-1978',\n",
       " '27-04-2000',\n",
       " '07-08-1969',\n",
       " '10-07-1999',\n",
       " '28-01-1999',\n",
       " '09-03-1981',\n",
       " '13-01-1963',\n",
       " '11-06-1986',\n",
       " '22-03-1965',\n",
       " '11-08-1986',\n",
       " '20-08-1984',\n",
       " '10-02-1990',\n",
       " '09-08-2007',\n",
       " '22-09-1946',\n",
       " '26-07-2000',\n",
       " '28-10-1989',\n",
       " '06-10-1979',\n",
       " '11-07-1991',\n",
       " '04-11-2004',\n",
       " '10-05-1988',\n",
       " '16-07-2008',\n",
       " '02-10-1994',\n",
       " '16-06-1976',\n",
       " '25-12-1967',\n",
       " '04-06-1967',\n",
       " '23-01-1993',\n",
       " '02-10-1956',\n",
       " '05-11-1992',\n",
       " '22-03-1979',\n",
       " '29-09-1972',\n",
       " '14-03-1993',\n",
       " '13-12-1989',\n",
       " '02-11-1989',\n",
       " '17-04-1970',\n",
       " '13-12-1994',\n",
       " '14-11-1972',\n",
       " '24-06-1993',\n",
       " '22-12-1993',\n",
       " '03-09-2001',\n",
       " '12-09-1980',\n",
       " '05-04-1974',\n",
       " '09-02-1967',\n",
       " '27-10-1978',\n",
       " '09-04-1966',\n",
       " '11-06-1982',\n",
       " '10-02-1992',\n",
       " '30-01-1986',\n",
       " '08-02-1978',\n",
       " '24-10-1976',\n",
       " '04-11-1999',\n",
       " '21-06-1996',\n",
       " '22-11-1979',\n",
       " '11-11-1975',\n",
       " '01-08-1973',\n",
       " '01-02-1993',\n",
       " '09-06-1974',\n",
       " '13-01-2011',\n",
       " '11-07-1983',\n",
       " '15-04-2012',\n",
       " '31-03-1977',\n",
       " '04-07-1987',\n",
       " '16-09-1990',\n",
       " '01-02-2000',\n",
       " '21-06-1976',\n",
       " '09-11-1997',\n",
       " '12-08-1979',\n",
       " '23-03-1992',\n",
       " '26-01-1979',\n",
       " '15-05-1957',\n",
       " '04-03-1991',\n",
       " '04-06-1994',\n",
       " '22-12-1986',\n",
       " '29-06-2002',\n",
       " '16-03-2004',\n",
       " '17-02-1993',\n",
       " '23-03-1986',\n",
       " '12-09-1996',\n",
       " '04-09-1991',\n",
       " '14-05-1998',\n",
       " '24-04-1968',\n",
       " '03-09-1998',\n",
       " '14-05-1992',\n",
       " '28-04-1991',\n",
       " '10-08-2010',\n",
       " '08-02-2003',\n",
       " '24-09-1960',\n",
       " '27-01-2004',\n",
       " '20-07-2007',\n",
       " '08-09-1955',\n",
       " '03-04-1991',\n",
       " '20-01-1999',\n",
       " '28-02-1977',\n",
       " '18-12-1980',\n",
       " '17-03-1962',\n",
       " '04-12-1965',\n",
       " '05-02-1996',\n",
       " '12-02-1962',\n",
       " '17-05-1991',\n",
       " '01-08-2000',\n",
       " '07-10-2003',\n",
       " '26-12-1975',\n",
       " '01-04-1996',\n",
       " '21-01-1973',\n",
       " '24-03-1983',\n",
       " '08-03-2005',\n",
       " '10-02-2002',\n",
       " '06-10-1995',\n",
       " '14-08-2005',\n",
       " '24-03-1992',\n",
       " '12-08-2001',\n",
       " '16-08-1998',\n",
       " '16-02-1968',\n",
       " '26-02-1998',\n",
       " '23-07-1995',\n",
       " '12-08-1985',\n",
       " '04-04-1970',\n",
       " '28-03-1985',\n",
       " '04-02-1980',\n",
       " '09-11-1978',\n",
       " '22-02-2000',\n",
       " '14-04-2001',\n",
       " '13-08-1973',\n",
       " '27-02-1996',\n",
       " '08-07-2004',\n",
       " '22-02-1995',\n",
       " '19-08-1970',\n",
       " '11-04-1974',\n",
       " '28-04-1998',\n",
       " '03-02-2004',\n",
       " '11-11-1978',\n",
       " '09-10-1996',\n",
       " '01-12-1946',\n",
       " '21-08-2002',\n",
       " '11-09-1995',\n",
       " '01-11-1978',\n",
       " '26-05-1984',\n",
       " '07-02-1977',\n",
       " '25-03-1980',\n",
       " '18-11-1995',\n",
       " '16-08-1974',\n",
       " '16-11-1989',\n",
       " '16-07-1972',\n",
       " '15-05-1990',\n",
       " '01-08-1988',\n",
       " '24-04-1968',\n",
       " '18-04-1984',\n",
       " '10-12-2004',\n",
       " '31-07-1960',\n",
       " '05-01-1982',\n",
       " '13-05-2001',\n",
       " '08-08-2001',\n",
       " '25-12-2002',\n",
       " '18-03-1975',\n",
       " '18-09-2000',\n",
       " '01-03-1979',\n",
       " '04-10-1999',\n",
       " '29-05-1986',\n",
       " '17-04-1976',\n",
       " '21-03-2008',\n",
       " '09-06-1967',\n",
       " '15-10-1982',\n",
       " '14-12-1970',\n",
       " '27-06-1986',\n",
       " '09-07-1999',\n",
       " '10-02-1998',\n",
       " '02-11-1975',\n",
       " '25-03-1967',\n",
       " '20-12-1999',\n",
       " '27-01-1985',\n",
       " '29-01-1984',\n",
       " '29-04-1996',\n",
       " '25-05-2004',\n",
       " '27-08-1955',\n",
       " '26-12-1994',\n",
       " '04-08-2001',\n",
       " '13-10-1974',\n",
       " '25-06-1966',\n",
       " '21-12-1981',\n",
       " '15-11-1998',\n",
       " '02-03-1990',\n",
       " '21-04-1972',\n",
       " '25-12-2003',\n",
       " '05-06-1984',\n",
       " '04-09-1980',\n",
       " '21-05-1979',\n",
       " '30-05-2005',\n",
       " '06-04-1978',\n",
       " '02-02-1950',\n",
       " '30-06-1986',\n",
       " '25-12-1999',\n",
       " '02-11-1999',\n",
       " '03-06-2009',\n",
       " '02-01-1989',\n",
       " '22-11-1970',\n",
       " '25-10-1987',\n",
       " '28-09-1991',\n",
       " '08-01-1970',\n",
       " '31-12-1989',\n",
       " '23-07-2000',\n",
       " '04-01-1979',\n",
       " '06-10-1995',\n",
       " '16-03-1973',\n",
       " '05-07-1967',\n",
       " '24-07-1966',\n",
       " '24-10-2003',\n",
       " '28-03-2003',\n",
       " '04-11-1988',\n",
       " '12-03-1994',\n",
       " '01-08-1987',\n",
       " '16-12-1995',\n",
       " '19-11-1983',\n",
       " '21-11-1998',\n",
       " '06-07-1994',\n",
       " '20-05-2003',\n",
       " '17-07-1984',\n",
       " '17-03-2002',\n",
       " '07-03-1974',\n",
       " '27-08-1995',\n",
       " '30-12-1984',\n",
       " '13-02-1990',\n",
       " '01-03-2005',\n",
       " '29-01-2002',\n",
       " '28-09-1999',\n",
       " '12-09-1979',\n",
       " '08-05-1969',\n",
       " '01-11-1985',\n",
       " '28-10-1979',\n",
       " '14-12-1992',\n",
       " '12-06-2001',\n",
       " '29-06-2002',\n",
       " '27-03-1982',\n",
       " '03-06-1976',\n",
       " '31-01-1989',\n",
       " '08-08-1969',\n",
       " '17-10-1958',\n",
       " '26-06-1989',\n",
       " '01-09-1956',\n",
       " '21-04-1988',\n",
       " '22-03-1972',\n",
       " '29-05-1993',\n",
       " '12-09-2002',\n",
       " '18-11-1985',\n",
       " '25-06-1983',\n",
       " '18-11-1978',\n",
       " '26-01-1991',\n",
       " '19-07-1988',\n",
       " '29-05-1978',\n",
       " '07-01-1998',\n",
       " '12-11-2012',\n",
       " '13-11-1990',\n",
       " '19-01-1990',\n",
       " '19-01-2003',\n",
       " '11-05-2001',\n",
       " '02-09-1989',\n",
       " '31-10-2001',\n",
       " '04-07-1973',\n",
       " '07-12-1987',\n",
       " '21-01-1989',\n",
       " '22-11-2012',\n",
       " '18-03-1985',\n",
       " '25-04-1982',\n",
       " '13-10-2000',\n",
       " '20-09-1991',\n",
       " '26-08-1955',\n",
       " '31-01-1996',\n",
       " '29-06-1989',\n",
       " '09-11-1994',\n",
       " '02-04-1976',\n",
       " '29-12-1971',\n",
       " '26-09-1988',\n",
       " '07-05-1995',\n",
       " '27-07-1977',\n",
       " '22-04-1980',\n",
       " '28-08-1984',\n",
       " '23-11-1983',\n",
       " '07-05-1993',\n",
       " '04-07-1986',\n",
       " '02-07-1981',\n",
       " '09-03-1994',\n",
       " '26-09-1982',\n",
       " '19-05-1996',\n",
       " '27-09-1983',\n",
       " '15-03-1999',\n",
       " '29-10-1960',\n",
       " '02-07-2001',\n",
       " '31-12-1971',\n",
       " '28-06-1971',\n",
       " '14-11-1993',\n",
       " '26-08-1969',\n",
       " '19-03-1985',\n",
       " '07-06-1990',\n",
       " '01-03-1966',\n",
       " '25-07-1995',\n",
       " '25-05-2004',\n",
       " '29-08-1991',\n",
       " '08-02-1993',\n",
       " '02-10-1996',\n",
       " '27-03-1968',\n",
       " '14-12-1991',\n",
       " '11-02-1966',\n",
       " '07-05-1986',\n",
       " '17-04-1973',\n",
       " '06-09-1996',\n",
       " '22-06-1961',\n",
       " '26-08-2000',\n",
       " '26-08-1980',\n",
       " '28-06-1970',\n",
       " '13-05-1999',\n",
       " '30-12-1995',\n",
       " '07-07-1997',\n",
       " '06-09-1995',\n",
       " '27-09-1977',\n",
       " '12-01-1989',\n",
       " '15-11-1970',\n",
       " '08-07-1973',\n",
       " '03-06-1993',\n",
       " '31-03-1990',\n",
       " '06-03-1969',\n",
       " '11-01-1999',\n",
       " '08-11-1976',\n",
       " '01-06-1996',\n",
       " '18-04-1979',\n",
       " '03-06-1959',\n",
       " '26-09-1969',\n",
       " '16-12-1967',\n",
       " '29-07-1983',\n",
       " '21-05-2004',\n",
       " '04-12-2000',\n",
       " '09-04-1993',\n",
       " '24-08-1987',\n",
       " '03-01-2001',\n",
       " '04-02-1998',\n",
       " '08-05-1994',\n",
       " '08-04-1991',\n",
       " '16-10-1995',\n",
       " '24-06-1990',\n",
       " '27-02-1971',\n",
       " '08-10-1973',\n",
       " '16-06-1980',\n",
       " '27-04-1998',\n",
       " '16-04-1987',\n",
       " '14-06-1991',\n",
       " '29-05-2003',\n",
       " '25-02-1980',\n",
       " '20-10-1961',\n",
       " '12-07-1994',\n",
       " '09-08-1986',\n",
       " '31-10-1979',\n",
       " '24-03-1987',\n",
       " '07-11-2003',\n",
       " '30-01-1983',\n",
       " '03-08-1993',\n",
       " '04-08-1980',\n",
       " '27-10-1994',\n",
       " '27-05-1983',\n",
       " '21-07-1976',\n",
       " '08-09-1958',\n",
       " '30-07-1969',\n",
       " '15-07-1966',\n",
       " '25-01-1974',\n",
       " '10-07-1985',\n",
       " '26-09-1956',\n",
       " '10-04-1979',\n",
       " '25-04-2000',\n",
       " '09-06-1986',\n",
       " '22-11-2008',\n",
       " '30-06-2000',\n",
       " '20-01-1990',\n",
       " '01-03-1989',\n",
       " '07-03-1978',\n",
       " '11-07-1989',\n",
       " '17-07-1985',\n",
       " '25-11-1995',\n",
       " '20-03-1996',\n",
       " '17-02-1997',\n",
       " '04-08-1981',\n",
       " '04-09-1993',\n",
       " '02-12-1985',\n",
       " '11-10-1993',\n",
       " '17-03-1998',\n",
       " '31-10-1978',\n",
       " '29-11-1997',\n",
       " '30-03-2000',\n",
       " '26-02-1987',\n",
       " '13-01-1996',\n",
       " '01-03-1977',\n",
       " '08-04-1994',\n",
       " '27-04-1976',\n",
       " '05-05-1973',\n",
       " '01-10-2013',\n",
       " '19-04-2001',\n",
       " '23-10-1984',\n",
       " '05-01-1993',\n",
       " '07-12-1948',\n",
       " '14-05-1991',\n",
       " '02-09-1980',\n",
       " '21-05-1994',\n",
       " '02-01-1969',\n",
       " '10-05-1992',\n",
       " '02-06-1970',\n",
       " '02-09-1949',\n",
       " '24-06-1998',\n",
       " '05-01-1990',\n",
       " '27-11-1993',\n",
       " '13-02-1996',\n",
       " '24-05-1998',\n",
       " '16-03-1999',\n",
       " '12-12-1998',\n",
       " '10-07-1979',\n",
       " '09-06-1985',\n",
       " '19-11-1969',\n",
       " '04-03-1996',\n",
       " '26-06-1984',\n",
       " '05-10-1987',\n",
       " '06-06-1993',\n",
       " '28-12-1995',\n",
       " '20-12-1991',\n",
       " '12-07-1978',\n",
       " '21-03-2003',\n",
       " '16-04-1979',\n",
       " '06-01-2005',\n",
       " '13-05-1989',\n",
       " '05-06-1960',\n",
       " '29-08-1998',\n",
       " '03-01-1999',\n",
       " '07-08-1971',\n",
       " '07-11-1991',\n",
       " '02-08-1972',\n",
       " '26-05-1972',\n",
       " '30-04-1978',\n",
       " '17-10-2002',\n",
       " '05-11-1991',\n",
       " '24-04-1989',\n",
       " '22-12-2002',\n",
       " '22-02-1985',\n",
       " '26-12-1982',\n",
       " '23-08-1963',\n",
       " '30-05-1966',\n",
       " '02-05-1980',\n",
       " '29-10-1968',\n",
       " '21-06-1959',\n",
       " '07-12-1994',\n",
       " '26-08-2003',\n",
       " '18-09-1976',\n",
       " '12-09-2001',\n",
       " '10-02-1991',\n",
       " '14-01-1960',\n",
       " '10-01-1991',\n",
       " '15-12-1996',\n",
       " '17-08-2005',\n",
       " '07-09-1986',\n",
       " '10-02-1984',\n",
       " '03-12-1965',\n",
       " '20-04-1993',\n",
       " '25-08-1969',\n",
       " '21-02-2014',\n",
       " '16-02-1979',\n",
       " '22-11-1998',\n",
       " '13-06-1983',\n",
       " '04-05-1966',\n",
       " '07-07-2001',\n",
       " '31-01-1973',\n",
       " '12-08-1988',\n",
       " '11-03-2005',\n",
       " '04-03-1979',\n",
       " '19-01-2005',\n",
       " '29-05-1999',\n",
       " '07-06-1995',\n",
       " '08-03-1977',\n",
       " '15-02-1984',\n",
       " '26-05-1983',\n",
       " '22-03-2015',\n",
       " '01-11-1964',\n",
       " '07-05-1992',\n",
       " '29-07-2001',\n",
       " '17-05-1988',\n",
       " '20-01-2004',\n",
       " '11-10-2001',\n",
       " '06-01-1996',\n",
       " '18-08-2003',\n",
       " '11-01-1999',\n",
       " '25-03-1987',\n",
       " '18-08-2000',\n",
       " '20-04-1996',\n",
       " '31-01-1975',\n",
       " '12-05-1979',\n",
       " '10-03-1989',\n",
       " '05-08-2003',\n",
       " '13-01-1968',\n",
       " '20-08-1978',\n",
       " '26-05-1982',\n",
       " '12-07-1981',\n",
       " '20-04-1996',\n",
       " '24-08-1998',\n",
       " '31-10-1979',\n",
       " '24-08-1996',\n",
       " '16-06-1987',\n",
       " '21-06-2006',\n",
       " '08-04-1986',\n",
       " '07-01-1997',\n",
       " '23-11-1986',\n",
       " '14-11-2001',\n",
       " '13-06-2003',\n",
       " '07-01-1979',\n",
       " '05-10-1992',\n",
       " '08-12-1999',\n",
       " '16-10-1990',\n",
       " '17-03-1980',\n",
       " '11-05-1968',\n",
       " '28-09-1971',\n",
       " '02-05-2003',\n",
       " '17-04-1984',\n",
       " '18-12-1995',\n",
       " '26-06-1987',\n",
       " '25-12-1966',\n",
       " '08-08-1966',\n",
       " '13-11-1999',\n",
       " '10-12-1988',\n",
       " '28-03-1994',\n",
       " '14-04-1986',\n",
       " '27-09-2002',\n",
       " '11-04-1999',\n",
       " '17-02-1987',\n",
       " '07-08-1987',\n",
       " '17-07-1990',\n",
       " '03-03-1977',\n",
       " '17-02-1997',\n",
       " '14-09-1978',\n",
       " '18-07-1999',\n",
       " '31-08-1996',\n",
       " '04-06-1978',\n",
       " '25-11-1988',\n",
       " '03-01-1988',\n",
       " '30-05-1985',\n",
       " '23-03-1973',\n",
       " '24-07-1978',\n",
       " '30-12-1979',\n",
       " '14-04-1995',\n",
       " '18-10-2000',\n",
       " '19-11-1976',\n",
       " '17-10-2003',\n",
       " '05-05-1992',\n",
       " '19-02-1990',\n",
       " '21-02-1991',\n",
       " '03-03-1976',\n",
       " '13-01-1994',\n",
       " '27-01-1987',\n",
       " '29-05-1979',\n",
       " '19-02-1993',\n",
       " '22-02-2001',\n",
       " '08-12-1987',\n",
       " '28-03-1994',\n",
       " '16-02-1990',\n",
       " '04-11-1976',\n",
       " '09-07-1989',\n",
       " '03-05-1981',\n",
       " '27-12-2000',\n",
       " '09-06-1999',\n",
       " '15-03-2010',\n",
       " '16-12-1957',\n",
       " '22-09-1997',\n",
       " '31-08-2004',\n",
       " '17-10-1984',\n",
       " '14-09-1972',\n",
       " '06-08-2002',\n",
       " '02-10-1958',\n",
       " '04-06-1978',\n",
       " '11-04-1995',\n",
       " '13-10-1995',\n",
       " '10-02-1989',\n",
       " '07-09-1991',\n",
       " '11-08-1985',\n",
       " '22-01-1988',\n",
       " '07-03-1989',\n",
       " '27-10-2001',\n",
       " '24-06-2000',\n",
       " '14-08-1978',\n",
       " '01-01-1960',\n",
       " '25-08-1988',\n",
       " '24-09-1969',\n",
       " '20-09-1977',\n",
       " '29-06-1979',\n",
       " '18-05-1992',\n",
       " '24-11-1993',\n",
       " '13-09-1997',\n",
       " '08-09-1965',\n",
       " '02-08-1982',\n",
       " '13-10-1974',\n",
       " '02-04-1975',\n",
       " '20-03-2002',\n",
       " '17-01-1991',\n",
       " '26-03-2001',\n",
       " '30-12-2001',\n",
       " '28-05-1999',\n",
       " '05-03-1978',\n",
       " '18-05-1995',\n",
       " '23-05-1990',\n",
       " '02-01-1997',\n",
       " '01-07-1989',\n",
       " '03-06-1981',\n",
       " '08-05-2004',\n",
       " '25-11-1980',\n",
       " '30-05-1989',\n",
       " '23-10-1967',\n",
       " '22-07-1998',\n",
       " '31-01-2001',\n",
       " '25-09-1994',\n",
       " '01-01-1964',\n",
       " '18-04-1967',\n",
       " '09-11-1973',\n",
       " '23-09-1990',\n",
       " '27-02-1984',\n",
       " '20-04-1999',\n",
       " '28-10-1991',\n",
       " '01-01-1957',\n",
       " '08-03-1994',\n",
       " '29-10-1957',\n",
       " '04-03-1980',\n",
       " '15-11-1994',\n",
       " '14-05-1981',\n",
       " '13-05-1977',\n",
       " '27-04-2001',\n",
       " '09-08-1996',\n",
       " '20-08-1998',\n",
       " '20-12-1995',\n",
       " '20-09-1990',\n",
       " '22-12-1978',\n",
       " '02-02-1997',\n",
       " '23-02-1990',\n",
       " '11-07-2014',\n",
       " '06-02-1973',\n",
       " '26-07-1990',\n",
       " '06-10-1975',\n",
       " '06-09-1974',\n",
       " '19-08-1997',\n",
       " '05-03-1978',\n",
       " '14-08-2003',\n",
       " '03-06-1998',\n",
       " '08-03-1984',\n",
       " '30-10-1990',\n",
       " '27-09-2003',\n",
       " '28-07-1986',\n",
       " '01-12-1986',\n",
       " '24-10-1975',\n",
       " '13-01-1982',\n",
       " '16-09-1965',\n",
       " '03-05-1996',\n",
       " '13-12-1988',\n",
       " '18-03-1928',\n",
       " '19-08-1982',\n",
       " '27-05-1970',\n",
       " '29-05-1984',\n",
       " '01-11-1966',\n",
       " '25-06-1985',\n",
       " '03-07-2003',\n",
       " '19-10-1983',\n",
       " '30-03-1986',\n",
       " '18-12-1965',\n",
       " '11-04-1996',\n",
       " '29-12-1996',\n",
       " '30-07-1989',\n",
       " '10-12-1968',\n",
       " '23-10-1970',\n",
       " '02-08-2003',\n",
       " '05-05-1983',\n",
       " '04-05-1966',\n",
       " '22-02-1977',\n",
       " '27-01-2000',\n",
       " '27-10-1972',\n",
       " '29-08-1996',\n",
       " '14-07-1989',\n",
       " '06-04-1971',\n",
       " '16-02-1964',\n",
       " '08-10-2005',\n",
       " '28-01-1985',\n",
       " '14-03-1974',\n",
       " '03-11-1989',\n",
       " '12-10-1967',\n",
       " '07-02-1978',\n",
       " '09-09-1996',\n",
       " '30-11-1993',\n",
       " '10-08-1998',\n",
       " '02-09-1985',\n",
       " '11-05-1977',\n",
       " '08-08-1991',\n",
       " '23-06-1982',\n",
       " '03-06-2001',\n",
       " '14-01-1972',\n",
       " '04-09-1988',\n",
       " '05-04-1988',\n",
       " '03-07-1989',\n",
       " '18-10-1984',\n",
       " '17-02-1984',\n",
       " '30-04-1971',\n",
       " '07-07-1985',\n",
       " '12-09-1980',\n",
       " '03-12-2001',\n",
       " '27-01-1971',\n",
       " '28-11-1995',\n",
       " '21-09-1985',\n",
       " '16-07-1984',\n",
       " '13-01-1987',\n",
       " '18-08-1971',\n",
       " '18-04-1999',\n",
       " '31-05-2001',\n",
       " '21-05-1964',\n",
       " '02-02-2005',\n",
       " '22-12-1979',\n",
       " '05-06-1997',\n",
       " '23-10-1997',\n",
       " '07-04-1957',\n",
       " '10-06-1999',\n",
       " '23-02-1975',\n",
       " '08-11-1966',\n",
       " '16-03-2001',\n",
       " '13-10-1977',\n",
       " '01-06-1988',\n",
       " '14-07-1980',\n",
       " '15-03-1995',\n",
       " '06-04-1975',\n",
       " '23-01-1983',\n",
       " '10-12-2001',\n",
       " '26-04-1982',\n",
       " '21-03-1978',\n",
       " '28-11-1971',\n",
       " '27-03-2005',\n",
       " ...]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial View of the Column\n",
    "birth_values = df_records['date_of_birth'].tolist()\n",
    "birth_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "871daca6-7a72-4039-9d97-d01887e63349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing (NaN) values: 0\n",
      "Percentage of missing values: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Count NaN values\n",
    "count_missing = df_records['date_of_birth'].isna().sum()\n",
    "missing_percentages = (count_missing / len(df_records)) * 100\n",
    "\n",
    "print(f\"Number of missing (NaN) values: {count_missing}\")\n",
    "print(f\"Percentage of missing values: {missing_percentages:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58b5df-9a6e-4138-8b0d-3efe459cdc93",
   "metadata": {},
   "source": [
    "# Clean days_since_request Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc7172-077b-4154-b051-5aeab74e4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "days_since_request_values = df_records['days_since_request'].unique\n",
    "days_since_request_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b3480-3e1e-44d0-9f5d-ef031bfd9f2a",
   "metadata": {},
   "source": [
    "For this column, there were many entries with decimal places, indicating less than a day had passed. For this, we decided to round these down to zero and create a new flag column to reference all those accounts with same day requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392c1d0-a570-4f06-ba29-6bb7c55c2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to whole days\n",
    "df_records['days_since_request'] = df_records['days_since_request'].round().astype('Int64')\n",
    "\n",
    "# Add a same-day flag\n",
    "df_records['same_day'] = df_records['days_since_request'] < 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea747f-84ca-4a14-91a6-08764fe3e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "days_since_request_values = df_records['days_since_request'].unique()\n",
    "days_since_request_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d958d96-37d6-4873-9bf1-83ebfe55982b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean intended_balcon_amount Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b6ae9-efda-4c78-8b27-478389dcf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "intended_balcon_amount_values = df_records['intended_balcon_amount'].tolist()\n",
    "intended_balcon_amount_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fca0d-0b37-44bf-8baf-34e9eb3fdc8b",
   "metadata": {},
   "source": [
    "For this column, we will treat all values less than 0 as missing entries. We also rounded all values to the nearest dollar amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750f246-aa98-4999-9342-863283877b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 0 decimals and replace negatives with -1\n",
    "df_records['intended_balcon_amount'] = df_records['intended_balcon_amount'].round(0)\n",
    "df_records['intended_balcon_amount'] = df_records['intended_balcon_amount'].where(df_records['intended_balcon_amount'] >= 0, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009453f-95dd-4ff7-83c9-70a5f1c9b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "intended_balcon_amount_values = df_records['intended_balcon_amount'].unique()\n",
    "intended_balcon_amount_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bac296-f371-4d67-b0ea-0df582f503cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -0.0 with 0.0\n",
    "df_records['intended_balcon_amount'] = df_records['intended_balcon_amount'].replace(-0.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d5128-d04f-41e4-9f4a-416fe79381dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean payment_type Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074644b-d53f-43cc-8963-409d6e4ba67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "payment_type_values = df_records['payment_type'].unique()\n",
    "payment_type_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098e54c-340b-4f07-aeec-51c9aa513b57",
   "metadata": {},
   "source": [
    "I asked Martin, the data scientist, about the mapping for this column. This is what I was provided with:\n",
    "From what I recall of the mapping table:\n",
    "\n",
    "AA → ACH / Direct Deposit\n",
    "\n",
    "AB → Debit or Credit Card\n",
    "\n",
    "AC → Check\n",
    "\n",
    "AD → Cash (branch funding)\n",
    "\n",
    "AE → Other / Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855a4a9-7467-49da-aead-d3a9ddfef56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping table\n",
    "payment_map = {\n",
    "    \"AA\": \"ACH / Direct Deposit\",\n",
    "    \"AB\": \"Debit or Credit Card\",\n",
    "    \"AC\": \"Check\",\n",
    "    \"AD\": \"Cash (Branch Funding)\",\n",
    "    \"AE\": \"Other / Unknown\"\n",
    "}\n",
    "\n",
    "# Apply mapping, default to \"Unknown\" if unmapped\n",
    "df_records['payment_type'] = df_records['payment_type'].map(payment_map).fillna(\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe57cff-07e6-430d-a7c0-051974f26706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "payment_type_values = df_records['payment_type'].unique()\n",
    "payment_type_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46174b2-784b-46b5-a33c-963c912ad406",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean zip_count_4w Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3879fb3-181c-4688-affd-6c1d2511d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "zip_count_4w_values = df_records['zip_count_4w'].tolist()\n",
    "zip_count_4w_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191430d-a5cc-47fa-a1da-cd85ecc3315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_None =  df_records['zip_count_4w'].isna()\n",
    "type_None.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80c301-6006-4ab2-867a-3e5c1a2c83ad",
   "metadata": {},
   "source": [
    "There does not seem to be anything missing or wrong with this column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c02544-b2ec-4a49-959a-468b14bb23a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean velocity_6h column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957eb00-3a8a-4b96-92bb-bba2b2370f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "velocity_6h_values = df_records['velocity_6h'].tolist()\n",
    "velocity_6h_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633eb213-091b-4164-a481-687eb6c14e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 0 decimals\n",
    "df_records['velocity_6h'] = df_records['velocity_6h'].round(0)\n",
    "\n",
    "# Replace negatives with -1\n",
    "df_records['velocity_6h'] = df_records['velocity_6h'].where(df_records['velocity_6h'] >= 0, -1)\n",
    "\n",
    "# Force clean integers (no -0.0, no float artifacts)\n",
    "df_records['velocity_6h'] = df_records['velocity_6h'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab630bfd-7fcc-4eff-ac8c-ef8018efe050",
   "metadata": {},
   "source": [
    "Since we are looking at applications, I removed all decimals to ensure we are representing all applications as a whole. You cannot have a .5 application. Furthemore, for all negative values, I changed the values to -1 to represent missing values or errors in the data. We can impute these in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd647266-45f0-498e-b365-1229a6e71324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "velocity_6h_values = df_records['velocity_6h']\n",
    "velocity_6h_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82330442-7aa7-4e02-b6d3-0f5a46b65d14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean velocity_24h column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57539b8a-a4d9-44f2-9ecc-69b57e549b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "velocity_24h_values = df_records['velocity_24h'].tolist()\n",
    "velocity_24h_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cc55f-aef6-424b-b7e3-081c35502e18",
   "metadata": {},
   "source": [
    "I am repeating the same process for this column as the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc74313-fe6e-4992-9d9e-572c6e211e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 0 decimals\n",
    "df_records['velocity_24h'] = df_records['velocity_24h'].round(0)\n",
    "\n",
    "# Replace negatives with -1\n",
    "df_records['velocity_24h'] = df_records['velocity_24h'].where(df_records['velocity_24h'] >= 0, -1)\n",
    "\n",
    "# Force clean integers (no -0.0, no float artifacts)\n",
    "df_records['velocity_24h'] = df_records['velocity_24h'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94669ea7-eb06-4b93-b1a2-55f9c6204c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "velocity_24h_values = df_records['velocity_24h'].tolist()\n",
    "velocity_24h_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9f377-8dbf-4666-8d97-79661f0fb0d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean velocity_4w Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726eca0-f33a-44a3-893e-f43772206ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "velocity_4w_values = df_records['velocity_4w'].tolist()\n",
    "velocity_4w_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db6b0c-9d7d-41f5-bc12-19b2a144c15b",
   "metadata": {},
   "source": [
    "I am repeating the same process for this column as the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "53efb2e5-4f8b-422f-a96f-7c203b51028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 0 decimals\n",
    "df_records['velocity_4w'] = df_records['velocity_4w'].round(0)\n",
    "\n",
    "# Replace negatives with -1\n",
    "df_records['velocity_4w'] = df_records['velocity_4w'].where(df_records['velocity_4w'] >= 0, -1)\n",
    "\n",
    "# Force clean integers (no -0.0, no float artifacts)\n",
    "df_records['velocity_4w'] = df_records['velocity_4w'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82883df6-a5a5-4108-8d76-4bf8a6a0fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial View of the Column\n",
    "velocity_4w_values = df_records['velocity_4w'].tolist()\n",
    "velocity_4w_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c8234-6a46-4f19-a8af-df2dad71b9fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean bank_branch_count_8w Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9de86-ea13-4778-ad43-a7362cbd8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_branch_count_8w_values = df_records['bank_branch_count_8w'].tolist()\n",
    "bank_branch_count_8w_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111f042-1f8a-48b7-a1e8-2d71f7b23b61",
   "metadata": {},
   "source": [
    "I'm changing this to an integer with no decimal places. There cannot be a .5 application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "104b2145-6151-49c7-a0bf-7535d2e0da3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           3\n",
       "1          11\n",
       "2           1\n",
       "3         705\n",
       "4          28\n",
       "         ... \n",
       "823546      9\n",
       "823547    744\n",
       "823548      0\n",
       "823549      7\n",
       "823550     14\n",
       "Name: bank_branch_count_8w, Length: 823551, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records['bank_branch_count_8w'] = df_records['bank_branch_count_8w'].astype(int)\n",
    "bank_branch_count_8w_values = df_records['bank_branch_count_8w']\n",
    "bank_branch_count_8w_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00629556-a9e7-4015-914e-1c6c6d077efb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean date_of_birth_distinct_emails_4w Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9799e6b0-5311-448a-85d7-d07bf7a9787b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 13.,  6.,  5.,  8.,  7., 10., 20.,  9., 22., 15., 14., 16.,\n",
       "        3., 11.,  2., 17.,  0., 12.,  4., 19.,  1., 23., 21., 28., 29.,\n",
       "       26., 25., 24., 27., 32., 30., 33., 31., 34., 36., 35., 37., 39.,\n",
       "       38.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_of_birth_distinct_emails_4w_values = df_records['date_of_birth_distinct_emails_4w']\n",
    "date_of_birth_distinct_emails_4w_values.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb448aa5-dbee-4f17-90f8-3fd03cae3b08",
   "metadata": {},
   "source": [
    "Chaning this cell to an integer data type, since these are adding whole applications together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "089aeef1-d6bf-4548-b899-a5637416fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records['date_of_birth_distinct_emails_4w'] = df_records['date_of_birth_distinct_emails_4w'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af673b84-0153-47b8-901e-985d4cb310d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean Employment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "01808c05-7668-4ab2-b95b-4580f5b58403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CA', 'CB', 'CC', 'CF', 'CD', 'CE', 'CG'], dtype=object)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employment_status_values = df_records['employment_status']\n",
    "employment_status_values.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9855f-2864-4caa-8725-83308d5de208",
   "metadata": {},
   "source": [
    "Based on what Martin said:\n",
    "\n",
    "I wasn’t able to find a formal data dictionary with the mapping inside the uploaded DFS documents. But based on how the employment codes were set up in the old intake system, here’s the mapping we used internally:\n",
    "\n",
    "CA → Employed full-time\n",
    "\n",
    "CB → Employed part-time\n",
    "\n",
    "CC → Self-employed\n",
    "\n",
    "CD → Student\n",
    "\n",
    "CE → Unemployed\n",
    "\n",
    "CF → Retired\n",
    "\n",
    "CG → Other / Not specified\n",
    "\n",
    "Those codes slipped into the dataset when raw system values weren’t translated back to text labels. For modeling and reporting, we always remapped them to plain categories, and anything unusual (like a stray code outside A–G) was bucketed into “Unknown.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dcc63e2a-f7b7-4972-95af-7d35cabc5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping dictionary\n",
    "employment_map = {\n",
    "    \"CA\": \"Employed full-time\",\n",
    "    \"CB\": \"Employed part-time\",\n",
    "    \"CC\": \"Self-employed\",\n",
    "    \"CD\": \"Student\",\n",
    "    \"CE\": \"Unemployed\",\n",
    "    \"CF\": \"Retired\",\n",
    "    \"CG\": \"Other / Not specified\"\n",
    "}\n",
    "\n",
    "# Apply mapping, default to \"Other / Not specified\" if unmapped\n",
    "df_records['employment_status'] = df_records['employment_status'].map(employment_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9902912-3a98-41ef-b827-f2bbb52b26e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Employed full-time', 'Employed part-time', 'Self-employed',\n",
       "       'Retired', 'Student', 'Unemployed', 'Other / Not specified'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employment_status_values = df_records['employment_status']\n",
    "employment_status_values.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c222a-f331-4788-b4ba-f744327025a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (web-scrapper)",
   "language": "python",
   "name": "web-scrapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
